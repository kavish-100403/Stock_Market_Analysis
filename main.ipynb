{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Collecting Data from Yfinance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start='2012-01-01'\n",
    "end='2022-12-21'\n",
    "\n",
    "stock='GOOG'\n",
    "\n",
    "data=yf.download(stock, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replacing Index from Dates to Numbers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>16.262545</td>\n",
       "      <td>16.641375</td>\n",
       "      <td>16.248346</td>\n",
       "      <td>16.573130</td>\n",
       "      <td>16.532528</td>\n",
       "      <td>147611217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>16.563665</td>\n",
       "      <td>16.693678</td>\n",
       "      <td>16.453827</td>\n",
       "      <td>16.644611</td>\n",
       "      <td>16.603836</td>\n",
       "      <td>114989399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>16.491436</td>\n",
       "      <td>16.537264</td>\n",
       "      <td>16.344486</td>\n",
       "      <td>16.413727</td>\n",
       "      <td>16.373516</td>\n",
       "      <td>131808205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>16.417213</td>\n",
       "      <td>16.438385</td>\n",
       "      <td>16.184088</td>\n",
       "      <td>16.189817</td>\n",
       "      <td>16.150156</td>\n",
       "      <td>108119746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-09</td>\n",
       "      <td>16.102144</td>\n",
       "      <td>16.114599</td>\n",
       "      <td>15.472754</td>\n",
       "      <td>15.503389</td>\n",
       "      <td>15.465409</td>\n",
       "      <td>233776981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>2022-12-14</td>\n",
       "      <td>95.540001</td>\n",
       "      <td>97.220001</td>\n",
       "      <td>93.940002</td>\n",
       "      <td>95.309998</td>\n",
       "      <td>95.076508</td>\n",
       "      <td>26452900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>93.540001</td>\n",
       "      <td>94.029999</td>\n",
       "      <td>90.430000</td>\n",
       "      <td>91.199997</td>\n",
       "      <td>90.976578</td>\n",
       "      <td>28298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>2022-12-16</td>\n",
       "      <td>91.199997</td>\n",
       "      <td>91.750000</td>\n",
       "      <td>90.010002</td>\n",
       "      <td>90.860001</td>\n",
       "      <td>90.637405</td>\n",
       "      <td>48485500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>90.879997</td>\n",
       "      <td>91.199997</td>\n",
       "      <td>88.925003</td>\n",
       "      <td>89.150002</td>\n",
       "      <td>88.931602</td>\n",
       "      <td>23020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>2022-12-20</td>\n",
       "      <td>88.730003</td>\n",
       "      <td>89.779999</td>\n",
       "      <td>88.040001</td>\n",
       "      <td>89.629997</td>\n",
       "      <td>89.410423</td>\n",
       "      <td>21976800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2761 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date       Open       High        Low      Close  Adj Close  \\\n",
       "0    2012-01-03  16.262545  16.641375  16.248346  16.573130  16.532528   \n",
       "1    2012-01-04  16.563665  16.693678  16.453827  16.644611  16.603836   \n",
       "2    2012-01-05  16.491436  16.537264  16.344486  16.413727  16.373516   \n",
       "3    2012-01-06  16.417213  16.438385  16.184088  16.189817  16.150156   \n",
       "4    2012-01-09  16.102144  16.114599  15.472754  15.503389  15.465409   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2756 2022-12-14  95.540001  97.220001  93.940002  95.309998  95.076508   \n",
       "2757 2022-12-15  93.540001  94.029999  90.430000  91.199997  90.976578   \n",
       "2758 2022-12-16  91.199997  91.750000  90.010002  90.860001  90.637405   \n",
       "2759 2022-12-19  90.879997  91.199997  88.925003  89.150002  88.931602   \n",
       "2760 2022-12-20  88.730003  89.779999  88.040001  89.629997  89.410423   \n",
       "\n",
       "         Volume  \n",
       "0     147611217  \n",
       "1     114989399  \n",
       "2     131808205  \n",
       "3     108119746  \n",
       "4     233776981  \n",
       "...         ...  \n",
       "2756   26452900  \n",
       "2757   28298800  \n",
       "2758   48485500  \n",
       "2759   23020500  \n",
       "2760   21976800  \n",
       "\n",
       "[2761 rows x 7 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating 100 days and 200 days days moving average of close price*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_100_days=data.Close.rolling(100).mean()\n",
    "ma_200_days=data.Close.rolling(200).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If there are any null values the data will be deleted*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Splitting the data into training and test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.DataFrame(data.Close[0:int(len(data)*0.8)])\n",
    "data_test = pd.DataFrame(data.Close[int(len(data)*0.8):len(data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Defining a function which will data between 0 and 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fitting the Training Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scale=scaler.fit_transform(data_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "for i in range(100,data_train_scale.shape[0]):\n",
    "    x.append(data_train_scale[i-100:i])\n",
    "    y.append(data_train_scale[i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Changing the list into an array for both X and Y*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Importing Keras Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating model Sequential and training it with LSTM layers where the mentioned units are the dimensions and the return _sequences means the output of first is the input for the second and defining the input shape*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=((x.shape[1],1))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=60, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(units=80, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(units=120, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(units=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compiling the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fitting the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "66/66 [==============================] - 13s 138ms/step - loss: 0.0304\n",
      "Epoch 2/70\n",
      "66/66 [==============================] - 10s 145ms/step - loss: 0.0074\n",
      "Epoch 3/70\n",
      "66/66 [==============================] - 10s 145ms/step - loss: 0.0055\n",
      "Epoch 4/70\n",
      "66/66 [==============================] - 9s 142ms/step - loss: 0.0051\n",
      "Epoch 5/70\n",
      "66/66 [==============================] - 10s 155ms/step - loss: 0.0046\n",
      "Epoch 6/70\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 0.0052\n",
      "Epoch 7/70\n",
      "66/66 [==============================] - 9s 139ms/step - loss: 0.0047\n",
      "Epoch 8/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0041\n",
      "Epoch 9/70\n",
      "66/66 [==============================] - 9s 138ms/step - loss: 0.0036\n",
      "Epoch 10/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0036\n",
      "Epoch 11/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0036\n",
      "Epoch 12/70\n",
      "66/66 [==============================] - 9s 138ms/step - loss: 0.0033\n",
      "Epoch 13/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0033\n",
      "Epoch 14/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0034\n",
      "Epoch 15/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0035\n",
      "Epoch 16/70\n",
      "66/66 [==============================] - 9s 139ms/step - loss: 0.0031\n",
      "Epoch 17/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0029\n",
      "Epoch 18/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0028\n",
      "Epoch 19/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0027\n",
      "Epoch 20/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0025\n",
      "Epoch 21/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0031\n",
      "Epoch 22/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0025\n",
      "Epoch 23/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0023\n",
      "Epoch 24/70\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 0.0026\n",
      "Epoch 25/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0026\n",
      "Epoch 26/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0027\n",
      "Epoch 27/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0024\n",
      "Epoch 28/70\n",
      "66/66 [==============================] - 9s 135ms/step - loss: 0.0024\n",
      "Epoch 29/70\n",
      "66/66 [==============================] - 9s 135ms/step - loss: 0.0022\n",
      "Epoch 30/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0019\n",
      "Epoch 31/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0021\n",
      "Epoch 32/70\n",
      "66/66 [==============================] - 9s 135ms/step - loss: 0.0022\n",
      "Epoch 33/70\n",
      "66/66 [==============================] - 9s 135ms/step - loss: 0.0022\n",
      "Epoch 34/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0021\n",
      "Epoch 35/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0019\n",
      "Epoch 36/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0021\n",
      "Epoch 37/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0021\n",
      "Epoch 38/70\n",
      "66/66 [==============================] - 9s 139ms/step - loss: 0.0020\n",
      "Epoch 39/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0022\n",
      "Epoch 40/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0021\n",
      "Epoch 41/70\n",
      "66/66 [==============================] - 9s 139ms/step - loss: 0.0018\n",
      "Epoch 42/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0020\n",
      "Epoch 43/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0020\n",
      "Epoch 44/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0020\n",
      "Epoch 45/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0021\n",
      "Epoch 46/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0019\n",
      "Epoch 47/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0019\n",
      "Epoch 48/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0019\n",
      "Epoch 49/70\n",
      "66/66 [==============================] - 9s 135ms/step - loss: 0.0020\n",
      "Epoch 50/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0022\n",
      "Epoch 51/70\n",
      "66/66 [==============================] - 9s 134ms/step - loss: 0.0019\n",
      "Epoch 52/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0018\n",
      "Epoch 53/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0019\n",
      "Epoch 54/70\n",
      "66/66 [==============================] - 9s 135ms/step - loss: 0.0018\n",
      "Epoch 55/70\n",
      "66/66 [==============================] - 9s 135ms/step - loss: 0.0018\n",
      "Epoch 56/70\n",
      "66/66 [==============================] - 10s 146ms/step - loss: 0.0021\n",
      "Epoch 57/70\n",
      "66/66 [==============================] - 9s 141ms/step - loss: 0.0019\n",
      "Epoch 58/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0019\n",
      "Epoch 59/70\n",
      "66/66 [==============================] - 9s 138ms/step - loss: 0.0018\n",
      "Epoch 60/70\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 0.0022\n",
      "Epoch 61/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0020\n",
      "Epoch 62/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0019\n",
      "Epoch 63/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0017\n",
      "Epoch 64/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0017\n",
      "Epoch 65/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0019\n",
      "Epoch 66/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0020\n",
      "Epoch 67/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0019\n",
      "Epoch 68/70\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.0018\n",
      "Epoch 69/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0019\n",
      "Epoch 70/70\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.0017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f5b5f024f0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y, epochs=70, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Collecting the last 100 days data of training in a variable*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_100_days=data_train.tail(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Concatinating the last 100 days data and the test data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653\n"
     ]
    }
   ],
   "source": [
    "data_test=pd.concat([pas_100_days,data_test], ignore_index=True)\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scaling the test data between 0 and 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_scale= scaler.fit_transform(data_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Array Slicing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "for i in range(100,data_test_scale.shape[0]):\n",
    "    x.append(data_test_scale[i-100:i])\n",
    "    y.append(data_test_scale[i,0])\n",
    "x,y=np.array(x), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Predicting the values of X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_predict=model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Descaling the vpredicted values and the original values for plotting of graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "descale =1/scaler.scale_\n",
    "y_predict= y_predict*descale\n",
    "y=y*descale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving this model in a keras file so it can be used by Streamlit for web*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Stock_Prediction_Model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
